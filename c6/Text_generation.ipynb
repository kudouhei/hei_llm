{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fscEyl4B1zwO"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install git+https://github.com/abetlen/llama-cpp-python.git"
      ],
      "metadata": {
        "id": "ijqjkzNh4xbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install langchain-community"
      ],
      "metadata": {
        "id": "m4w8Ibco4ik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG5d8Wvx2Fq6",
        "outputId": "94c84a0e-b05c-463d-94ed-98dcd057708c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TEdNHxJs6FsH",
        "outputId": "ba29380a-2ff0-4df4-d248-b71180ac3155"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"input_prompt\"])"
      ],
      "metadata": {
        "id": "HtzYboPx6KoA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = prompt | llm"
      ],
      "metadata": {
        "id": "T2L7GmOt6X2n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "v11h0snX6aQx",
        "outputId": "4766341a-447c-4992-bbc8-f2a924a53de3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Chains"
      ],
      "metadata": {
        "id": "MLluwkOo6qe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ],
      "metadata": {
        "id": "PoAM7Wmy6saQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title.invoke({\"summary\": \"a dog that lost its mother\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O-LTTbI7BDH",
        "outputId": "99b9cb76-b80e-4bb3-b8a1-fd0159f5767b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a dog that lost its mother',\n",
              " 'title': ' \"The Lone Pup: A Tale of Finding Strength After Mother\\'s Absence\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\"])\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ],
      "metadata": {
        "id": "E4AqMGe07Sv6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ],
      "metadata": {
        "id": "qOMaXrvc7f6F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ],
      "metadata": {
        "id": "INiSxQNp7lso"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a dog that lost its mother\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu7Wmh417qcj",
        "outputId": "f0feec02-bd5b-435e-d2bb-27534cd44504"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a dog that lost its mother',\n",
              " 'title': ' The Lone Journey of Bella: Navigating Life Without Momma.',\n",
              " 'character': \" Bella, a spirited and resilient golden retriever, finds herself on an arduous journey after losing her beloved mother in a tragic accident. Her unwavering determination to survive and adapt fuels her lone journey as she navigates the challenges of life without Momma's warm embrace and guidance.\\n\\nThe main character, Bella, is portrayed with emotional depth and courage that captures both readers' hearts as they follow her solitary path toward growth and self-discovery in a world where companionship has been taken away from her by the loss of Momma.\",\n",
              " 'story': \" In the heart of a bustling city, Bella, an intrepid golden retriever with sparkling amber eyes and a glossy coat that shone like sunlight on water, faced her lone journey after losing her beloved mother in a tragic accident. With unwavering determination and resilience etched into her spirit, she embarked upon life's challenges without the comfort of Momma's warm embrace or guidance. Bella braved the harsh realities of city streets, learning to fend for herself while seeking companionship among kind strangers who helped mold her courage and heartfelt gratitude into an unbreakable bond with humanity. Through every trial, she became a beacon of inspiration, embodying the strength that sprang from navigating life's arduous path soloâ€”a testament to love, loss, and the undying will to forge ahead without Momma by her side.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "ZJOGFnjk_q8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "YLVbJPTm_vNo",
        "outputId": "7d3248d8-e94a-44e5-f245-3fe7663839e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "g1jPD0VXDebF",
        "outputId": "e087a6d1-5f53-4b60-d4fd-a1334826707f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name as I don't have access to personal data of individuals. If you need assistance with something specific, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationBuffer"
      ],
      "metadata": {
        "id": "QU7KOL8zDqaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ],
      "metadata": {
        "id": "SAfLT62GDuLL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6UV-W7TDyni",
        "outputId": "4087ef7b-1e46-4585-8894-2157d89726f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-9823403f20ac>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sRHTE5gD5x4",
        "outputId": "89ebdfbd-1a17-43cc-f96b-c515a46cddee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Regarding your question, the answer to 1 + 1 is indeed 2. It's always good to revisit fundamental math concepts like theseâ€”simple addition forms the foundation for more complex calculations later on. Anything else you'd like to know or discuss today?\\n\\nIf this were a standalone message:\\n\\nHello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrjaN8eWE0VB",
        "outputId": "86fec7f1-8fe4-4a47-fc85-ff28383c51b7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Regarding your question, the answer to 1 + 1 is indeed 2. It's always good to revisit fundamental math concepts like theseâ€”simple addition forms the foundation for more complex calculations later on. Anything else you'd like to know or discuss today?\\n\\nIf this were a standalone message:\\n\\nHello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\",\n",
              " 'text': \" Hello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\\n\\nYour name is Maarten as mentioned at the beginning of this conversation.\\n\\n-----\"}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAEUDGMlFQ-X",
        "outputId": "4d509524-e9cd-4f9a-8573-15881ec6df29"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-046ef635f261>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBnopd2GFTWM",
        "outputId": "ba980441-791c-4ac2-d94d-b2ff7d3ae1a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\",\n",
              " 'text': ' The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZY9SeJEFuMr",
        "outputId": "c2ed62f6-ccca-4884-8d75-1333fd0ddf62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\nHuman: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!\",\n",
              " 'text': \" Your name is Maarten.\\n\\nHowever, the information you provided about your name was in the initial conversation and not directly repeated here. If you're asking from this part of our current interaction, then I don't have that detail yet as it wasn't previously mentioned after my responses.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9dElBkVGSFU",
        "outputId": "8b219a50-13df-4a1d-d0bd-a775c7dddc84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!\\nHuman: What is my name?\\nAI:  Your name is Maarten.\\n\\nHowever, the information you provided about your name was in the initial conversation and not directly repeated here. If you're asking from this part of our current interaction, then I don't have that detail yet as it wasn't previously mentioned after my responses.\",\n",
              " 'text': \" I'm an AI and do not have the ability to know personal information about individuals, including your age. This data is private, and you may choose to share it when you feel comfortable doing so in a secure manner. If you need assistance with calculating something or understanding concepts related to time, I can help explain those!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationSummary"
      ],
      "metadata": {
        "id": "ezcIQJHwG0lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "EC0q46n0G2yj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-qBVk3vG7-n",
        "outputId": "27c1fb3d-8493-49ef-daa7-ccad396c2c8a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-905e44fca19a>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlLyps5mHEcl",
        "outputId": "f413fe7b-9a11-40d3-a40b-e5f6d93bf6a0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \" Two cities that might experience an upsurge in diplomatic activities, taking into account the mentioned factors, are Istanbul, Turkey and Seoul, South Korea.\\n\\n\\nIstanbul's geographical significance as a gateway between continents positions it favorably for negotiations related to the Middle East and European-Asian relations, despite current regional challenges. Its historical role in commerce and culture makes it an ideal venue for international diplomacy.\\n\\n\\nSeoul is at the forefront of technological advancements, particularly in renewable energy, which aligns with global efforts to combat climate change. South Korea's vibrant economy and active cultural exchange programs also contribute to its potential as a nexus for diplomatic engagement, especially considering its alliances and economic partnerships that could be bolstered by dialogue on sustainable technologies.\",\n",
              " 'text': \" Your name is not explicitly provided in the current conversation, so it's impossible to determine from this context. The information shared is focused on discussing the potential of Istanbul and Seoul as cities for diplomatic activities based on their geographical significance, historical roles, technological advancements, cultural exchanges, economic standing, and regional alliances.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg6TUjPGKqAX",
        "outputId": "f9da1cf5-cab6-49e1-b131-050acd18bdcd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': \" Considering the discussed factors, Istanbul and Seoul are identified as two cities with significant potential for increased diplomatic activities. Istanbul's strategic location between Europe and Asia makes it a key player in Middle Eastern and European-Asian relations, while its rich history of trade and culture provides a strong foundation for international diplomacy despite regional challenges. On the other hand, Seoul stands out as a leader in technological innovation, particularly renewable energy solutions, which align with global sustainability efforts. South Korea's robust economy and active cultural exchange initiatives, coupled with its strategic alliances, further underscore its suitability as a hub for diplomatic discussions focusing on green technology and economic partnerships.\",\n",
              " 'text': ' It appears there was no direct question preceding the detailed explanation about Istanbul and Seoul\\'s potential for increased diplomatic activities. Based on your description, a possible initial question could have been: \"What are some cities that show significant potential for enhanced diplomatic activities due to their strategic locations or unique strengths?\" This question sets the stage for the discussion you provided about Istanbul and Seoul.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27snYbIoL3Dj",
        "outputId": "103ba68f-78f3-4859-8956-52245ce457c9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': \" The conversation revolves around identifying cities with significant potential for enhanced diplomatic activities, focusing on Istanbul and Seoul. Istanbul's strategic location between Europe and Asia positions it as a key player in Middle Eastern and European-Asian relations, offering opportunities for international diplomacy despite regional challenges. Its rich history of trade and culture strengthens its suitability for global diplomatic engagements. Conversely, Seoul is recognized for its leadership in technological innovation, especially renewable energy solutions, aligning with global sustainability goals. South Korea's strong economy, cultural exchanges, and strategic alliances make it an ideal hub for discussions on green technology and economic partnerships.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "dwj_1aySL7yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install langchain-openai"
      ],
      "metadata": {
        "id": "B7-ysTIIMH3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U duckduckgo-search"
      ],
      "metadata": {
        "id": "sBlKDceZMWaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load OpenAI's LLMs with LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"my_key\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "gYE6JjDZL9aU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ReAct template\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ],
      "metadata": {
        "id": "s2wbkK51MPnL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# You can create the tool to pass to an agent\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# Prepare tools\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "tools.append(search_tool)"
      ],
      "metadata": {
        "id": "tOttdTgaMRYc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# Construct the ReAct agent\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "r9b5CU79MfYg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoTraAfPMkIb",
        "outputId": "8166ff12-a597-4db5-9d88-bcac0eec1a27"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the current price of a MacBook Pro in USD and then convert it to EUR using the given exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: A MacBook Pro cost ranges from just over $1000 to well over $3000. Determining the true cost for the MacBook Pro you need requires close examination of the model options, hardware configurations, and Apple's pricing strategies. Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch, title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: If you want the latest MacBook Pro with a larger display, you'll also have to upgrade to the 14-core M4 Pro chip. ... Best price (current) Best price (all-time) M2 MacBook Air (13-inch) $999: $799 ..., title: Best MacBook Deals: Save Big on the Latest M4 Laptops. Plus ... - CNET, link: https://www.cnet.com/deals/best-macbook-deals/, snippet: As of February 2025, the latest MacBook Pro 14-inch M4 model started with a retail price of 1,599 U.S. dollars while the 16-inch M4 Pro version began with a retail price of 2,499 U.S. dollars., title: Apple MacBook Pro price comparison 2025 - Statista, link: https://www.statista.com/statistics/1360426/apple-macbook-pro-price-comparison/, snippet: - 16â€³ M4 Pro MacBook Pro (24GB/512GB): $2199, $300 off MSRP - 16â€³ M4 Pro MacBook Pro (48GB/512GB): $2599, $300 off MSRP - (2): Best Buy is offering a $300 discount on Apple's new 16-inch M4 Pro and M4 Max MacBook Pros as well. Sale prices for online orders only, in-store prices may vary., title: Here are the lowest prices for 16-inch M4 MacBook Pros among Apple's ..., link: https://www.macprices.net/2024/11/30/here-are-the-lowest-prices-for-16-inch-m4-macbook-pros-among-apples-retailers-this-black-friday-cyber-monday-weekend/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD, now I need to convert it to EUR using the exchange rate provided.\n",
            "Action: Calculator\n",
            "Action Input: 2499 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The current price of a MacBook Pro in USD is $2499. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 2124.15 EUR.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The current price of a MacBook Pro in USD is $2499. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 2124.15 EUR.'}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}