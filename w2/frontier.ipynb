{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087ca19-7d40-47da-aa8b-26d8d2659c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f3981-a944-447c-9627-527e27806140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6ef42-45d7-447d-b0a9-ebdaa572f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefebe6-0142-4c06-909a-46147fa16b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25007d87-8af3-4e54-b549-ad8724058d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c540c-5f89-4028-9527-e52e259e0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1dc2-094f-4fd8-b03f-e0fb0f91f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ec76f-b9c4-4aa3-8582-c341d4596b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "# Low temperature (close to 0): The model becomes more deterministic. This results in more predictable and conservative outputs.\n",
    "# High temperature (close to 1 or above): The model becomes more random. This leads to more diverse and creative outputs\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679a044-36ae-448d-8769-d1b2b2195e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ec6b9-e5c2-4b7e-9885-09ad16b623b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ab2fbc-733c-4160-8253-d8da7854a1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, data scientists, here's one for you:\n",
      "\n",
      "Why did the data scientist break up with the scatter plot?\n",
      "\n",
      "... Because they said their relationship was too *correlated* and it wasn't showing any *independent variables*! \n",
      "\n",
      "Hope you got a chuckle out of that! Let me know if you want another!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9deea4b6-15b6-462c-a2d6-d8ec4cd85459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b23969-e76a-4de3-986e-b68bd5ce52e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several factors. Here's a structured approach to help you make this decision:\n",
       "\n",
       "### 1. Nature of the Problem\n",
       "- **Text-Intensive Tasks**: LLMs excel in understanding and generating human-like text. If your problem involves tasks such as text generation, summarization, translation, or sentiment analysis, an LLM might be suitable.\n",
       "- **Complex Language Understanding**: Problems requiring understanding of nuanced, complex language or context may benefit from LLM capabilities.\n",
       "\n",
       "### 2. Data Availability\n",
       "- **Text Data**: Ensure you have access to sufficient and relevant text data that the LLM can be trained or fine-tuned on.\n",
       "- **Quality and Diversity**: The data should be of high quality and diverse enough to cover various aspects of the problem.\n",
       "\n",
       "### 3. Problem Complexity\n",
       "- **Pattern Recognition**: If the problem requires identifying patterns or insights from unstructured text data, LLMs can be effective.\n",
       "- **Dynamic and Evolving Contexts**: LLMs are suitable for problems where the context or language frequently changes, and the model needs to adapt.\n",
       "\n",
       "### 4. Scalability and Resources\n",
       "- **Computational Resources**: LLMs require significant computational power. Ensure that you have the necessary infrastructure or are willing to use cloud-based solutions.\n",
       "- **Cost-Effectiveness**: Consider if the potential benefits outweigh the costs of implementing and maintaining an LLM solution.\n",
       "\n",
       "### 5. Integration and Usability\n",
       "- **Integration with Existing Systems**: Evaluate if the LLM can be easily integrated into your existing workflows or systems.\n",
       "- **User Accessibility**: Consider how end-users will interact with the LLM and ensure it enhances rather than complicates their tasks.\n",
       "\n",
       "### 6. Ethical and Legal Considerations\n",
       "- **Data Privacy**: Ensure compliance with data protection regulations and consider the privacy implications of using LLMs.\n",
       "- **Bias and Fairness**: Be aware of the potential biases in LLMs and plan for ways to mitigate these issues.\n",
       "\n",
       "### 7. Expected Outcomes\n",
       "- **Clear Objectives**: Define what success looks like for the LLM solution and ensure it aligns with your business goals.\n",
       "- **Measurable Impact**: Establish metrics to measure the impact of the LLM on your problem.\n",
       "\n",
       "### Conclusion\n",
       "If your business problem aligns well with these factors, it may be suitable for an LLM solution. However, it's crucial to conduct a pilot project or proof of concept to validate the effectiveness of the LLM before full-scale implementation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11afc977-ca4f-43d4-bbac-c525595050c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
