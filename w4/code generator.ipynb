{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84776637-bd09-4db8-8b05-3c514cc8aa12",
   "metadata": {},
   "source": [
    "code generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6955cf8-8e5f-4416-be3e-103ddbd0a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6248cfd6-5b90-4efa-8e1f-77ecd47c8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "openai = OpenAI()\n",
    "google.generativeai.configure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ff606a-8d13-47fc-a3d9-9e7f4a39f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4121dc-f86c-4e1a-b89f-babc34f73c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the scatter plot?\n",
      "\n",
      "Because they said they needed some *space*!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca286d04-29f0-4b79-ae1a-56a3d149bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series analyst?\n",
      "\n",
      "Because she said he was too focused on the past, and she wanted someone with a little more... future-oriented perspective! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bea1d7-3c62-4f85-8948-3756b68ddd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "# NOTE - option to use ultra-low cost models by uncommenting last 2 lines\n",
    "\n",
    "# openai = OpenAI()\n",
    "# claude = anthropic.Anthropic()\n",
    "# OPENAI_MODEL = \"gpt-4o\"\n",
    "# CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270ac814-079e-4a37-84cd-950fa3c209bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. \"\n",
    "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046eb19e-1466-4ef6-910d-1d255550ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98979be8-5204-4b46-a0c5-81dd282c13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19a151b-38d3-4fcc-966e-0b090660bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(cpp):\n",
    "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee530e5-4bf0-45a9-afa9-4cf033f8ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gpt(python):\n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        print(chunk)\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        print(fragment, end='', flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f352c5d3-4aae-4a29-b93a-6fbb4c6d508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gemini(python):\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "        api_key=google_api_key, \n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    stream = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        max_tokens=2000,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "        stream=True,\n",
    "    )\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        print(fragment, end='', flush=True)\n",
    "    write_output(reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb1445ca-82b3-42bf-874f-56faa8430683",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed17a89b-c853-4ad5-938d-091fb95575f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 7.733360 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5dcb00-7934-40c1-ace8-6e8558b45a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_gpt(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2244847-0c4d-49fe-9483-b7419ad8321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 8.351400 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a18a4c4-b7a3-454a-8edf-9f70fdf249e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      " long long param2) {g long iterations, long long param1,\n",
      "    double result = 1.0;\n",
      ") { for (long long i = 1; i <= iterations; ++i\n",
      "        double j = static_cast<double>(i) * param1 - param2;\n",
      ");      result -= (1.0 / j\n",
      "        j = static_cast<double>(i) * param1 + param2;\n",
      "        result += (1.0 / j);\n",
      "    }\n",
      "    return result;\n",
      "}\n",
      "\n",
      "int main() {\n",
      " start_time = chrono::high_resolution_clock::now();\n",
      "    double result = calculate(100000000LL, 4LL, 1LL) * 4.0;\n",
      "auto end_time = chrono::high_resolution_clock::now();\n",
      "\n",
      "    auto duration = chrono::duration_cast<chrono::microseconds>(end_time - start_time);\n",
      ".0; double execution_time = static_cast<double>(duration.count()) / 1000000\n",
      "\n",
      "    cout << fixed << setprecision(12) << \"Result: \" << result << endl;\n",
      "    cout << fixed << setprecision(6) << \"Execution Time: \" << execution_time << \" seconds\" << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_gemini(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e9f6851-20ff-49ac-b6c4-7db27427c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 0.229879 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compile C++ and run the executable\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6f8a83b-34f5-4f86-889f-dfafb6a646db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      " std; namespace\n",
      "\n",
      "2) {le calculate(long long iterations, int param1, int param\n",
      "    double result = 1.0;\n",
      "    for (long long i = 1; i <= iterations; ++i) {\n",
      "double j = static_cast<double>(i) * param1 - param2;\n",
      "        result -= (1.0 / j);\n",
      " = static_cast<double>(i) * param1 + param2;\n",
      "        result += (1.0 / j);\n",
      "    }\n",
      "    return result;\n",
      "}\n",
      "\n",
      "int main() {\n",
      " chrono::high_resolution_clock::now();\n",
      "    double result = calculate(100000000LL, 4, 1) * 4;\n",
      "resolution_clock::now();no::high_\n",
      "\n",
      "    auto duration = chrono::duration_cast<chrono::microseconds>(end_time - start_time);\n",
      "    double execution_time = duration.count() / 1000000.0; // Seconds\n",
      "\n",
      "    cout << fixed << setprecision(12);\n",
      "    cout << \"Result: \" << result << endl;\n",
      "    cout << fixed << setprecision(6);\n",
      "    cout << \"Execution Time: \" << execution_time << \" seconds\" << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_gemini(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7003cde-e928-495d-836c-b860c0ad43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 0.229340 seconds\n"
     ]
    }
   ],
   "source": [
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0703a279-962c-47cc-ae11-e8252e0b2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66a75d13-404d-46ed-81d9-80124c9704e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 24.370670 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccc170bd-e088-420f-bc43-f030e90a7046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "include <iostream>\n",
      "#include <vector>\n",
      "#include <limits>\n",
      "#include <chrono>\n",
      "#include <iomanip>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      " LCG function\n",
      "unsigned int lcg(unsigned int& value, unsigned int a, unsigned int c, unsigned int m) {\n",
      "static_cast<unsigned long long>(a) * value + c) % m;\n",
      "    return value;\n",
      "}\n",
      "\n",
      "// Max Subarray Sum function\n",
      "long long max_subarray_sum(int n, unsigned int seed, int min_val, int max_val) {\n",
      "    vector<int> random_numbers(n);\n",
      "    unsigned int current_seed = seed;\n",
      "    for (int i = 0; i < n; ++i) {\n",
      "23, 4294967295) % (max_val - min_val + 1) + min_val;5, 10139042\n",
      "    }\n",
      "\n",
      "    long long max_sum = numeric_limits<long long>::min();\n",
      " < n; ++i) { i = 0; i\n",
      "        long long current_sum = 0;\n",
      "        for (int j = i; j < n; ++j) {\n",
      "            current_sum += random_numbers[j];\n",
      "            if (current_sum > max_sum) {\n",
      " current_sum;   max_sum =\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    return max_sum;\n",
      "}\n",
      "\n",
      "// Total Max Subarray Sum function\n",
      "long long total_max_subarray_sum(int n, unsigned int initial_seed, int min_val, int max_val) {\n",
      " long total_sum = 0;\n",
      "    unsigned int current_seed = initial_seed;\n",
      "    for (int i = 0; i < 20; ++i) {\n",
      "904223, 4294967295); seed = lcg(current_seed, 1664525, 1013\n",
      "        total_sum += max_subarray_sum(n, seed, min_val, max_val);\n",
      "    }\n",
      "    return total_sum;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    // Parameters\n",
      "    int n = 10000;          // Number of random numbers\n",
      "    unsigned int initial_seed = 42; // Initial seed for the LCG\n",
      "    int min_val = -10;       // Minimum value of random numbers\n",
      ";        // Maximum value of random numbers\n",
      "\n",
      "    // Timing the function\n",
      "    auto start_time = chrono::high_resolution_clock::now();\n",
      "    long long result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
      " chrono::high_resolution_clock::now();\n",
      "\n",
      "    auto duration = chrono::duration_cast<chrono::microseconds>(end_time - start_time);\n",
      "    double seconds = duration.count() / 1000000.0;\n",
      "\n",
      "0 runs): \" << result << endl;barray Sum (2\n",
      "    cout << \"Execution Time: \" << fixed << setprecision(6) << seconds << \" seconds\" << endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_gemini(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a22ae4a-eeaa-4da2-af9b-dcf39fa78fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 3480\n",
      "Execution Time: 0.666850 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23e29744-b46a-4a4f-9247-cd4df29419ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4123dcd-1a9f-40d0-935f-fe3ac8557572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini(python):\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "        api_key=google_api_key, \n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    stream = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        max_tokens=2000,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "        stream=True,\n",
    "    )\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bed0d5bc-e5cc-4ccf-b0c8-dcf25ba6af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46178bae-3037-43bf-bae6-8d317205e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a833fdf8-c346-4c56-b696-d369b2fac99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c5976a1a-8395-411f-ad3e-626b63038ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to change the code in the try block to compile the C++ code for your platform\n",
    "# I pasted this into Claude's chat UI with a request for it to give me a version for an Intel PC,\n",
    "# and it responded with something that looks perfect - you can try a similar approach for your platform.\n",
    "\n",
    "# M1 Mac version to compile and execute optimized C++ code:\n",
    "\n",
    "def execute_cpp(code):\n",
    "        write_output(code)\n",
    "        try:\n",
    "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
    "            compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "            run_cmd = [\"./optimized\"]\n",
    "            run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "            return run_result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c2647f-13d1-4f68-ad59-288346937e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a373a-a607-4bde-8ff9-402ab331959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da412a-f2c7-443c-9dd5-38b65e815485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your-key-if-not-using-env\n",
      "your-key-if-not-using-env\n",
      "your-key-if-not-using-env\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        cpp_run = gr.Button(\"Run C++\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4143adff-daa0-44f4-9e6d-c386eb35971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0742fc17-9ff4-4509-b7f2-0f2e107ccda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "testkey = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "907df783-9f25-466e-88e3-4ec6d0338cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "517eed40-7909-4bff-a122-1314de827d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_qwen = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
    "code_gemma = \"google/codegemma-7b-it\"\n",
    "\n",
    "# need huggingface model deploy \n",
    "# use models pay\n",
    "CODE_QWEN_URL = \"https://h1vdol7jxhje3mpn.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "CODE_GEMMA_URL = \"https://c5hggiyqachmgnqg.us-east-1.aws.endpoints.huggingface.cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d75eec4-70ad-4a90-bbff-768229ad66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(code_qwen)\n",
    "messages = messages_for(pi)\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2bd8f9-e937-447c-9778-71e618820e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. The C++ response needs to produce an identical output in the fastest possible time.<|im_end|>\n",
      "<|im_start|>user\n",
      "Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. Respond only with C++ code; do not explain your work other than a few comments. Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\n",
      "\n",
      "\n",
      "import time\n",
      "\n",
      "def calculate(iterations, param1, param2):\n",
      "    result = 1.0\n",
      "    for i in range(1, iterations+1):\n",
      "        j = i * param1 - param2\n",
      "        result -= (1/j)\n",
      "        j = i * param1 + param2\n",
      "        result += (1/j)\n",
      "    return result\n",
      "\n",
      "start_time = time.time()\n",
      "result = calculate(100_000_000, 4, 1) * 4\n",
      "end_time = time.time()\n",
      "\n",
      "print(f\"Result: {result:.12f}\")\n",
      "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6d0b34b-0093-475f-94c8-4709d55ff41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: sUk1Cp)\n\n403 Forbidden: None.\nCannot access content at: https://h1vdol7jxhje3mpn.us-east-1.aws.endpoints.huggingface.cloud/.\nMake sure your token has the correct permissions.\nForbidden: You don't have the required permissions to complete this action, missing permissions: inference.endpoints.infer.write",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://h1vdol7jxhje3mpn.us-east-1.aws.endpoints.huggingface.cloud/",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(CODE_QWEN_URL, token\u001b[38;5;241m=\u001b[39mhf_token)\n\u001b[0;32m----> 2\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mtoken\u001b[38;5;241m.\u001b[39mtext, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2356\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2331\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[1;32m   2332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2333\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   2334\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2354\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[1;32m   2355\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/inference/_common.py:410\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2326\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2326\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2328\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:321\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:472\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    467\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    475\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: sUk1Cp)\n\n403 Forbidden: None.\nCannot access content at: https://h1vdol7jxhje3mpn.us-east-1.aws.endpoints.huggingface.cloud/.\nMake sure your token has the correct permissions.\nForbidden: You don't have the required permissions to complete this action, missing permissions: inference.endpoints.infer.write"
     ]
    }
   ],
   "source": [
    "# need huggingface model deploy\n",
    "\n",
    "client = InferenceClient(CODE_QWEN_URL, token=hf_token)\n",
    "stream = client.text_generation(text, stream=True, details=True, max_new_tokens=3000)\n",
    "for r in stream:\n",
    "    print(r.token.text, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "106442d4-a47b-409f-ba70-d51b3ad4152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_program(sample_program):\n",
    "    if sample_program==\"pi\":\n",
    "        return pi\n",
    "    elif sample_program==\"python_hard\":\n",
    "        return python_hard\n",
    "    else:\n",
    "        return \"Type your Python program here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6614fdf1-b3ed-4560-b1ba-a891b339e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "VISUAL_STUDIO_2022_TOOLS = \"C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\Common7\\Tools\\\\VsDevCmd.bat\"\n",
    "VISUAL_STUDIO_2019_TOOLS = \"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\Common7\\\\Tools\\\\VsDevCmd.bat\"\n",
    "\n",
    "simple_cpp = \"\"\"\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"Hello\";\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def run_cmd(command_to_run):\n",
    "    try:\n",
    "        run_result = subprocess.run(command_to_run, check=True, text=True, capture_output=True)\n",
    "        return run_result.stdout if run_result.stdout else \"SUCCESS\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def c_compiler_cmd(filename_base):\n",
    "    my_platform = platform.system()\n",
    "    my_compiler = []\n",
    "\n",
    "    try:\n",
    "        with open(\"simple.cpp\", \"w\") as f:\n",
    "            f.write(simple_cpp)\n",
    "            \n",
    "        if my_platform == \"Windows\":\n",
    "            if os.path.isfile(VISUAL_STUDIO_2022_TOOLS):\n",
    "                if os.path.isfile(\"./simple.exe\"):\n",
    "                    os.remove(\"./simple.exe\")\n",
    "                compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Windows\", \"Visual Studio 2022\", [\"cmd\", \"/c\", VISUAL_STUDIO_2022_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "        \n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(VISUAL_STUDIO_2019_TOOLS):\n",
    "                    if os.path.isfile(\"./simple.exe\"):\n",
    "                        os.remove(\"./simple.exe\")\n",
    "                    compile_cmd = [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", \"simple.cpp\"]\n",
    "                    if run_cmd(compile_cmd):\n",
    "                        if run_cmd([\"./simple.exe\"]) == \"Hello\":\n",
    "                            my_compiler = [\"Windows\", \"Visual Studio 2019\", [\"cmd\", \"/c\", VISUAL_STUDIO_2019_TOOLS, \"&\", \"cl\", f\"{filename_base}.cpp\"]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "                \n",
    "        elif my_platform == \"Linux\":\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"g++\", \"simple.cpp\", \"-o\", \"simple\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Linux\", \"GCC (g++)\", [\"g++\", f\"{filename_base}.cpp\", \"-o\", f\"{filename_base}\" ]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                if os.path.isfile(\"./simple\"):\n",
    "                    os.remove(\"./simple\")\n",
    "                compile_cmd = [\"clang++\", \"simple.cpp\", \"-o\", \"simple\"]\n",
    "                if run_cmd(compile_cmd):\n",
    "                    if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                        my_compiler = [\"Linux\", \"Clang++\", [\"clang++\", f\"{filename_base}.cpp\", \"-o\", f\"{filename_base}\"]]\n",
    "        \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "    \n",
    "        elif my_platform == \"Darwin\":\n",
    "            if os.path.isfile(\"./simple\"):\n",
    "                os.remove(\"./simple\")\n",
    "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", \"simple\", \"simple.cpp\"]\n",
    "            if run_cmd(compile_cmd):\n",
    "                if run_cmd([\"./simple\"]) == \"Hello\":\n",
    "                    my_compiler = [\"Macintosh\", \"Clang++\", [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", f\"{filename_base}\", f\"{filename_base}.cpp\"]]\n",
    "    \n",
    "            if not my_compiler:\n",
    "                my_compiler=[my_platform, \"Unavailable\", []]\n",
    "    except:\n",
    "        my_compiler=[my_platform, \"Unavailable\", []]\n",
    "        \n",
    "    if my_compiler:\n",
    "        return my_compiler\n",
    "    else:\n",
    "        return [\"Unknown\", \"Unavailable\", []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4cdab-4089-4b72-bbe3-e75e9c31dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_cmd = c_compiler_cmd(\"optimized\")\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            sample_program = gr.Radio([\"pi\", \"python_hard\"], label=\"Sample program\", value=\"python_hard\")\n",
    "            model = gr.Dropdown([\"GPT\", \"Claude\", \"CodeQwen\"], label=\"Select model\", value=\"GPT\")\n",
    "        with gr.Column():\n",
    "            architecture = gr.Radio([compiler_cmd[0]], label=\"Architecture\", interactive=False, value=compiler_cmd[0])\n",
    "            compiler = gr.Radio([compiler_cmd[1]], label=\"Compiler\", interactive=False, value=compiler_cmd[1])\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        if not compiler_cmd[1] == \"Unavailable\":\n",
    "            cpp_run = gr.Button(\"Run C++\")\n",
    "        else:\n",
    "            cpp_run = gr.Button(\"No compiler to run C++\", interactive=False)\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "\n",
    "    sample_program.change(select_sample_program, inputs=[sample_program], outputs=[python])\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898439af-e908-4dee-9fd0-78318a5eae5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
